# Install required packages
# pip install langchain openai

# pip install langchain openai faiss-cpu pandas
#import pinecone
from dotenv import load_dotenv
# Load the .env file
load_dotenv("api_keys.env")

import os
import pandas as pd
from langchain import OpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

openai_api_key = os.getenv("OPENAI_API")
# Step 1: Initialize OpenAI API key
os.environ["OPENAI_API_KEY"] = openai_api_key

# Step 2: Load and preprocess your documents
from langchain.text_splitter import RecursiveCharacterTextSplitter


# Load and preprocess the CSV data
def load_and_preprocess_csv(file_path):
    df = pd.read_csv(file_path)
    text_data = df.to_csv(index=False)  # Convert the DataFrame to CSV format (text)
    return text_data

# Load and preprocess the .txt file
def load_and_preprocess_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text_data = file.read()  # Read the entire content of the .txt file
    return text_data

txt_file_path = r"C:\Users\sai.SMITHI\Downloads\J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt"
text_data = load_and_preprocess_txt(txt_file_path)


# Split the text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_text(text_data)

from langchain.embeddings import OpenAIEmbeddings

# Create an embedding using OpenAI
embeddings = OpenAIEmbeddings()



# Step 3: Create embeddings and store in a vector database
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(chunks, embeddings)


# Step 4: Set up the retriever and generator
retriever = vectorstore.as_retriever()
llm = OpenAI(model="gpt-3.5-turbo")  # Use an appropriate OpenAI model


# Step 5: Combine them into a RAG pipeline
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"  # You can use different chain types depending on your needs
)

from groq import Groq

groq_api_key = os.getenv("GROQ_API")
# Step 1: Initialize OpenAI API key
os.environ["GROQ_API_KEY"] = groq_api_key

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"))


from langchain_groq import ChatGroq

llm_groq = ChatGroq(
    model="mixtral-8x7b-32768",
    temperature=0,
    max_tokens=500,
    timeout=None,
    max_retries=2,
    # other params...
)


# Step 1: Retrieve relevant context based on the query
query_groq = "Who is harry potter"
retrieved_contexts = retriever.get_relevant_documents(query_groq)  # This returns relevant chunks based on the query

print(type(retrieved_contexts))
print(type(retrieved_contexts[0]))


# Print the first 3 retrieved documents
for i, doc in enumerate(retrieved_contexts[:3]):
    print(f"Document {i+1}:")
    print(doc.page_content)  # Adjust 'page_content' based on how the text is stored in the document
    print()  # Print a newline for better readability


# Step 2: Check the type of retrieved_documents and extract text
# Ensure that 'page_content' or similar field contains the text data
if isinstance(retrieved_contexts, list):
    context_text = "\n".join([doc.page_content for doc in retrieved_contexts[:3]])  # Extract first 3 document texts
else:
    raise TypeError(f"Expected a list of documents, but got {type(retrieved_contexts)}")

# Print the first 3 documents to verify
print("First 3 documents context:\n", context_text)

from langchain_core.prompts import ChatPromptTemplate

# Define the prompt template using system and human messages
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that answers questions according to the context. The context is: {context_text}.",
        ),
        ("human", "{query_groq}"),
    ]
)



# Step 2: Create a prompt incorporating the retrieved context
#prompt = f"Based on the following information, answer the question:\n\nContext:\n{context_text}\n\nQuestion: {query_groq}"

chain = prompt | llm_groq
chain.invoke(
    {
        "query_groq": query_groq,
        "context_text": context_text,
    }
)
